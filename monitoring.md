Вас пригласили настроить мониторинг на проект. На онбординге вам рассказали, что проект представляет из себя платформу для вычислений с выдачей текстовых отчетов, которые сохраняются на диск. Взаимодействие с платформой осуществляется по протоколу http. Также вам отметили, что вычисления загружают ЦПУ. Какой минимальный набор метрик вы выведите в мониторинг и почему?
Решение:
1. Загрузка процессора (CPU Usage)
Почему важно: Поскольку вычисления загружают ЦПУ, мониторинг нагрузки на процессор критически важен. Это поможет отслеживать, насколько загружен процессор, и своевременно реагировать на перегрузки.
Что мониторить: Среднюю загрузку ЦПУ, процент использования ЦПУ на уровне системы, процент использования ЦПУ для каждого процесса, а также количество активных процессов.
2. Использование памяти (Memory Usage)
Почему важно: Вычисления могут потреблять много оперативной памяти, и ее нехватка может привести к замедлению работы системы или ее отказу.
Что мониторить: Общий объем используемой памяти, свободная память, использование подкачки (swap), а также использование памяти на процесс.
3. Доступное дисковое пространство (Disk Usage)
Почему важно: Отчеты сохраняются на диск, и если место на диске закончится, это приведет к сбоям в работе системы.
Что мониторить: Объем свободного места на диске, использование диска в процентах, скорость записи и чтения данных с диска (I/O), а также наличие ошибок чтения/записи.
4. Время отклика HTTP (HTTP Response Time)
Почему важно: Платформа взаимодействует через HTTP, и время отклика критически важно для оценки производительности и пользовательского опыта.
Что мониторить: Время ответа на запросы (latency), успешные и неуспешные запросы (HTTP 2xx, 4xx, 5xx коды ответов), процент ошибок, количество запросов в единицу времени (throughput).
5. Нагрузка на дисковую подсистему (Disk I/O)
Почему важно: Запись текстовых отчетов на диск требует мониторинга операций ввода-вывода. Высокая нагрузка на диск может замедлить процесс записи или вызвать отказ.
Что мониторить: Количество операций чтения/записи в секунду, среднее время выполнения операции ввода-вывода, и использование IOPS (Input/Output Operations Per Second).
6. Состояние HTTP сервиса (HTTP Service Availability)
Почему важно: Доступность HTTP сервиса должна контролироваться, чтобы быть уверенным, что платформа работает корректно.
Что мониторить: Процент времени доступности сервиса, время простоев, наличие ошибок соединения, и число активных соединений.
7. Температура процессора (CPU Temperature)
Почему важно: В условиях высокой загрузки ЦПУ температура может повышаться до критических уровней, что может привести к троттлингу или аппаратным сбоям.
Что мониторить: Температура ядра процессора, допустимые максимальные значения, динамика изменения температуры.
8. Логи ошибок и системные события (Error Logs and System Events)
Почему важно: Анализ логов позволяет выявить сбои и аномалии в работе системы, особенно связанные с HTTP запросами или вычислениями.
Что мониторить: Автоматический сбор и анализ логов приложений и системы, логирование ошибок, отказов, аномалий.
9. Использование сети (Network Usage)
Почему важно: Поскольку взаимодействие происходит по HTTP, необходимо мониторить сетевую активность, чтобы убедиться в отсутствии сетевых проблем.
Что мониторить: Скорость передачи данных, объем переданных и полученных данных, количество активных соединений, ошибки передачи.
10. Очереди задач (Task Queues)
Почему важно: Если используется механизм очередей для обработки задач, важно следить за состоянием очередей, чтобы убедиться, что они не перегружены.
Что мониторить: Длина очередей, время ожидания задач в очереди, количество задач в обработке.

Менеджер продукта посмотрев на ваши метрики сказал, что ему непонятно что такое RAM/inodes/CPUla. Также он сказал, что хочет понимать, насколько мы выполняем свои обязанности перед клиентами и какое качество обслуживания. Что вы можете ему предложить?
Решение:
Можно предложить следующее:

Метрики уровня SLA (Service Level Agreement)
Время ответа на запросы (Response Time): Измеряет среднее время ответа на HTTP-запросы. Это поможет оценить, насколько быстро система реагирует на пользовательские запросы.
Процент выполнения запросов без ошибок (Success Rate): Доля успешных запросов (HTTP 2xx) по сравнению с общим числом запросов. Важный показатель для понимания качества обслуживания.
Время доступности системы (Uptime): Процент времени, когда система была доступна. Эта метрика показывает, насколько мы выполняем свои обязательства по обеспечению постоянной доступности сервиса.
2. Метрики, ориентированные на пользователя
Ошибки на уровне пользователей (User-Level Errors): Количество запросов, приведших к ошибкам (HTTP 4xx/5xx). Например, ошибки 5xx указывают на проблемы на сервере, которые непосредственно влияют на пользователей.
Среднее время обработки задачи (Task Processing Time): Если клиенты ожидают завершения вычислений, важно отслеживать среднее время выполнения этих задач и сравнивать его с ожидаемыми показателями.
3. Показатели пользовательского опыта (User Experience Metrics)
Латентность (Latency): Время задержки между запросом и ответом системы. Высокая латентность может указывать на ухудшение качества обслуживания.
Потребительская удовлетворенность (Customer Satisfaction, CSAT): Если есть опросы или система оценок, можно отслеживать уровень удовлетворенности клиентов, получая обратную связь о качестве обслуживания.
4. Инциденты и жалобы
Количество инцидентов (Incidents): Число зарегистрированных инцидентов или жалоб от клиентов за определенный период. Важно показывать динамику и время на устранение инцидентов.
Среднее время решения инцидента (Incident Resolution Time): Показатель того, насколько быстро решаются проблемы, с которыми сталкиваются клиенты.
5. Производительность системы под нагрузкой
Скорость обслуживания пиковых нагрузок (Peak Load Handling): Способность системы обрабатывать запросы в периоды максимальной активности. Важно, чтобы производительность не падала при увеличении числа запросов.
Презентация данных
Для менеджера продукта можно представить данные в виде графиков и отчетов, которые показывают:
Тренды: Например, изменение времени отклика или процент успешных запросов в течение дня, недели или месяца.
Целевые показатели: Сравнение фактических метрик с целями, установленными в SLA.
Прогнозы и предупреждения: Автоматические уведомления о превышении допустимых уровней времени отклика, увеличении числа ошибок или снижении доступности.

Вашей DevOps команде в этом году не выделили финансирование на построение системы сбора логов. Разработчики в свою очередь хотят видеть все ошибки, которые выдают их приложения. Какое решение вы можете предпринять в этой ситуации, чтобы разработчики получали ошибки приложения?
Решение:
Конфигурация логирования на уровне приложения
Консольное логирование: Настройте приложения так, чтобы они выводили ошибки и важные события в стандартный поток вывода (stdout) и поток ошибок (stderr). Эти потоки можно легко перенаправить и просматривать в режиме реального времени через SSH или через терминал.
Уровни логирования: Убедитесь, что в приложениях настроены правильные уровни логирования (ERROR, WARN, INFO, DEBUG). Логируйте ошибки и критические события на уровне ERROR, чтобы легче было фильтровать важную информацию.
2. Использование системных журналов (syslog)
Логирование в syslog: Перенаправьте логи приложений в системный журнал (syslog). Это позволит централизованно управлять логами на сервере. Syslog также позволяет фильтровать и пересылать определенные типы сообщений на другие серверы, если в будущем появится возможность развернуть систему централизованного логирования.
Использование логротаторов: Настройте logrotate для управления логами, чтобы старые логи автоматически архивировались или удалялись, предотвращая переполнение диска.
3. Email и уведомления
Уведомления по Email: Настройте приложения или скрипты для отправки email-уведомлений разработчикам при возникновении критических ошибок. Это можно сделать с использованием стандартных почтовых утилит, таких как sendmail или mailx.
Уведомления через Slack/Webhooks: Если команда использует Slack или другие системы уведомлений, можно отправлять важные ошибки через вебхуки, что позволит получать уведомления в режиме реального времени.
4. Просмотр логов в реальном времени
SSH доступ: Разрешите разработчикам доступ к серверам через SSH с ограниченными правами для просмотра логов в режиме реального времени с помощью команд, таких как tail -f.
Интеграция с простыми веб-интерфейсами: Если доступ к серверам ограничен, можно использовать простые инструменты вроде tailon или log.io, которые позволяют разработчикам просматривать логи через веб-интерфейс.
5. Простые скрипты для анализа логов
Фильтрация и агрегация: Напишите простые скрипты на Python/Bash, которые фильтруют логи по ключевым словам (например, ERROR или Exception) и отправляют их по email или сохраняют в отдельный файл, который будет проще анализировать.
Периодические отчеты: Настройте крон-задачи, которые периодически анализируют логи и отправляют сводки ошибок разработчикам.
6. Использование open-source решений
Lightweight решения: Рассмотрите легковесные open-source инструменты для сбора логов, такие как Fluentd, Filebeat или Graylog, которые могут быть настроены с минимальными затратами и могут работать на одном сервере.
7. Логирование в базы данных
База данных для ошибок: Если доступна база данных, можно настроить приложение так, чтобы оно логировало ошибки в отдельную таблицу. Разработчики смогут просматривать и фильтровать ошибки через SQL-запросы.

Вы, как опытный SRE, сделали мониторинг, куда вывели отображения выполнения SLA=99% по http кодам ответов. Вычисляете этот параметр по следующей формуле: summ_2xx_requests/summ_all_requests. Данный параметр не поднимается выше 70%, но при этом в вашей системе нет кодов ответа 5xx и 4xx. Где у вас ошибка?
Решение:
В данной ситуации ошибка в подходе к вычислению SLA, связанная с тем, что формула SLA = summ_2xx_requests / summ_all_requests предполагает, что в системе могут быть только успешные (2xx) и неуспешные (4xx и 5xx) HTTP-коды ответов. Однако, если у вас в системе нет ответов 4xx и 5xx, но SLA все равно ниже 70%, то проблема, вероятно, заключается в наличии других HTTP-кодов ответов, которые вы не учитываете.

Возможные коды ответов, влияющие на SLA:
Коды перенаправлений (3xx):

Ответы с кодами 3xx (например, 301, 302, 303, 307, 308) указывают на перенаправления. Они не считаются успешными запросами, но также и не являются ошибками. Однако их наличие в большом количестве может существенно снизить расчетный SLA, поскольку они не включаются в summ_2xx_requests.
Коды информационных ответов (1xx):

Ответы с кодами 1xx (например, 100 Continue, 101 Switching Protocols) также не считаются успешными запросами, но могут присутствовать в системе.
Как исправить расчет SLA:
Чтобы правильно вычислить SLA, нужно учесть только коды, которые действительно относятся к успешным операциям. Формула должна выглядеть следующим образом:
SLA = summ_2xx_requests / (summ_all_requests - summ_1xx_requests - summ_3xx_requests)

Дополнительные шаги:
Проверьте наличие запросов с кодами 3xx и 1xx. Эти коды могут существенно снижать расчетный SLA, хотя не являются ошибками.
Пересчитайте SLA, исключив из общего числа запросов (summ_all_requests) коды 1xx и 3xx. Это даст более точное представление о доле успешных запросов.
Анализируйте и обрабатывайте перенаправления и другие неуспешные статусы. Возможно, такие коды являются частью функциональности системы, и их следует учитывать при оценке SLA, или же они указывают на проблемные аспекты конфигурации системы.

Опишите основные плюсы и минусы pull и push систем мониторинга.

Решение:
Pull системы мониторинга
Плюсы:

Централизованное управление:

Сервер мониторинга (например, Prometheus) сам инициирует запросы, что упрощает управление частотой сбора метрик и позволяет гибко настраивать опрос для разных источников.
Легкость добавления новых источников данных:

Для добавления нового источника достаточно настроить его экспортер, и сервер мониторинга автоматически начнет собирать метрики.
Изоляция клиентов:

Клиенты не знают о наличии мониторинга, что упрощает конфигурацию и не требует внесения изменений в приложения для отправки метрик.
Автономность серверов:

Серверы могут быть настроены на сбор метрик даже при нестабильной сети, так как клиентские узлы не зависят от доступности сервера мониторинга.
Масштабируемость:

Сервер сам решает, когда и как часто опрашивать источники данных, что может облегчить балансировку нагрузки.
Минусы:

Проблемы с доступностью клиентов:

Если узел временно недоступен, сервер мониторинга не сможет получить метрики, что может привести к пропуску данных.
Сложность конфигурации для динамических сред:

В средах с динамическим созданием и удалением узлов (например, Kubernetes) требуется дополнительная конфигурация для обнаружения новых узлов.
Нагрузка на сервер мониторинга:

Серверу мониторинга нужно самостоятельно опрашивать всех клиентов, что может привести к повышенной нагрузке при большом количестве узлов.
Push системы мониторинга
Плюсы:

Легкость интеграции:

Приложениям просто отправлять метрики на указанный адрес, не требуется сложная настройка на стороне сервера.
Гибкость и контроль со стороны клиента:

Клиенты могут контролировать, когда и как часто отправлять метрики, что может быть полезно в реальных сценариях.
Легкость масштабирования:

В случае увеличения числа клиентов нагрузка распределяется по клиентам, а не концентрируется на сервере.
Поддержка динамических сред:

Подходит для динамически меняющихся окружений, где узлы могут часто добавляться или удаляться, так как каждый узел сам отправляет данные.
Минусы:

Требуется конфигурация на клиенте:

Все клиенты должны быть настроены на отправку метрик, что может быть сложно в крупных и распределенных системах.
Проблемы с доставкой данных:

Если сервер мониторинга временно недоступен, клиентские метрики могут быть потеряны или требуется настраивать буферизацию на стороне клиента.
Сложность управления частотой метрик:

Контроль за частотой отправки метрик лежит на клиенте, что может привести к избыточным или недостаточным данным.
Безопасность и конфиденциальность:

Клиенты отправляют данные напрямую на сервер, что требует дополнительного внимания к безопасности передачи данных.
Гибридные системы
Некоторые системы поддерживают и pull, и push модели, что позволяет выбрать оптимальный способ мониторинга в зависимости от конкретных задач и особенностей инфраструктуры. Гибридный подход может предоставить лучшие из обоих миров, но требует более сложной конфигурации и управления.

Какие из ниже перечисленных систем относятся к push модели, а какие к pull? А может есть гибридные?

Prometheus
TICK
Zabbix
VictoriaMetrics
Nagios
Решение:
Push модели
TICK (Telegraf, InfluxDB, Chronograf, Kapacitor):
В основном push модель, где Telegraf отправляет данные в InfluxDB.
Pull модели
Prometheus:

Использует pull модель. Prometheus периодически запрашивает метрики у источников (экспортеров).
VictoriaMetrics:

Основная модель — pull, аналогичная Prometheus. Однако VictoriaMetrics также поддерживает push, например, через Prometheus pushgateway.
Zabbix:

Работает в pull модели. Zabbix сервер опрашивает агентов на узлах для получения данных, но также может работать в push режиме через Zabbix Sender.
Nagios:

Преимущественно pull модель, где сервер периодически опрашивает агентов и проверяет доступные сервисы. Однако с использованием NRPE (Nagios Remote Plugin Executor) и других агентов возможна и push модель.
Гибридные модели
Некоторые из этих систем могут работать в гибридных режимах:

VictoriaMetrics: Основная pull модель, но может поддерживать push данные через различные методы (например, Prometheus pushgateway).
Zabbix: Основная pull модель, но поддерживает push с помощью Zabbix Sender.

Склонируйте себе репозиторий и запустите TICK-стэк, используя технологии docker и docker-compose.
В виде решения на это упражнение приведите скриншот веб-интерфейса ПО chronograf (http://localhost:8888).
P.S.: если при запуске некоторые контейнеры будут падать с ошибкой - проставьте им режим Z, например ./data:/var/lib:Z

Перейдите в веб-интерфейс Chronograf (http://localhost:8888) и откройте вкладку Data explorer.

Нажмите на кнопку Add a query
Изучите вывод интерфейса и выберите БД telegraf.autogen
В measurments выберите cpu->host->telegraf-getting-started, а в fields выберите usage_system. Внизу появится график утилизации cpu.
Вверху вы можете увидеть запрос, аналогичный SQL-синтаксису. Поэкспериментируйте с запросом, попробуйте изменить группировку и интервал наблюдений.
Для выполнения задания приведите скриншот с отображением метрик утилизации cpu из веб-интерфейса.

<img width="709" alt="image" src="https://github.com/user-attachments/assets/c87c0c92-79a2-4d32-9cd6-903214e9f0e7">


<img width="915" alt="image" src="https://github.com/user-attachments/assets/b7cd0da5-e7a5-4138-a39d-da83ffa6eb95">

<img width="1131" alt="image" src="https://github.com/user-attachments/assets/244c5290-6bb6-467d-b8cb-7921c3439ea4">


Изучите список telegraf inputs. Добавьте в конфигурацию telegraf следующий плагин - docker:
[[inputs.docker]]
  endpoint = "unix:///var/run/docker.sock"
Дополнительно вам может потребоваться донастройка контейнера telegraf в docker-compose.yml дополнительного volume и режима privileged:

  telegraf:
    image: telegraf:1.4.0
    privileged: true
    volumes:
      - ./etc/telegraf.conf:/etc/telegraf/telegraf.conf:Z
      - /var/run/docker.sock:/var/run/docker.sock:Z
    links:
      - influxdb
    ports:
      - "8092:8092/udp"
      - "8094:8094"
      - "8125:8125/udp"
После настройке перезапустите telegraf, обновите веб интерфейс и приведите скриншотом список measurments в веб-интерфейсе базы telegraf.autogen . Там должны появиться метрики, связанные с docker.

Факультативно можете изучить какие метрики собирает telegraf после выполнения данного задания.
Решение:

<img width="703" alt="image" src="https://github.com/user-attachments/assets/1758f0c6-cc8a-4ff0-a9ac-27e54ebd0c7c">
